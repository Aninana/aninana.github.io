# robots.txt Configuration File

# Basic rules applicable to all web crawlers
User-agent: *
# Allowed paths
Allow: /  
 
# Disallowed paths
Disallow: /archives/
Disallow: /categories/
Disallow: /tags/
Disallow: /images/

# General sitemap applicable to all crawlers (may be overridden by specific crawler rules)
# Note: Some crawlers may prioritize rules set specifically for them over general rules
Sitemap: https://blog.linux-qitong.top/sitemap.xml

# Special rules for Baidu Spider
User-agent: Baiduspider
# Provide a dedicated sitemap for Baidu Spider
Sitemap: https://blog.linux-qitong.top/baidusitemap.xml
